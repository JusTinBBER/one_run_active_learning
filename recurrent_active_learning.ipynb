{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用到的套件\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import os  \n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import torchvision\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torchvision import datasets,transforms\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import time\n",
    "import argparse\n",
    "from time import sleep\n",
    "from tqdm import tqdm, trange\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as FUN\n",
    "from scipy import io\n",
    "import efficientnet_pytorch\n",
    "import torchvision.transforms as T\n",
    "import PIL\n",
    "import pickle\n",
    "import torchvision.datasets as dsets\n",
    "from scipy.misc import imsave\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定義function\n",
    "\n",
    "def load_file(filename):\n",
    "    with open(filename, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='latin1')\n",
    "    return data\n",
    "\n",
    "# 解壓縮，返回解壓後的字典\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "#補邊,填充成正方形，防止resize變形\n",
    "def expend_img(img):\n",
    "    '''\n",
    "    :param img: 图片数据\n",
    "    :return:\n",
    "    '''\n",
    "    fill_pix=[0,0,0] #填充色素，可自己設定\n",
    "    h,w=img.shape[:2]\n",
    "    if h>=w: #左右填充\n",
    "        padd_width=int(h-w)//2\n",
    "        padd_top,padd_bottom,padd_left,padd_right=0,0,padd_width,padd_width #各個方向的填充像素\n",
    "    elif h<w: #上下填充\n",
    "        padd_high=int(w-h)//2\n",
    "        padd_top,padd_bottom,padd_left,padd_right=padd_high,padd_high,0,0 #各個方向的填充像素\n",
    "    new_img = cv2.copyMakeBorder(img,padd_top,padd_bottom,padd_left,padd_right,cv2.BORDER_CONSTANT, value=fill_pix)\n",
    "    return new_img\n",
    "\n",
    "#對影像做基本的旋轉 和亮度 對比度 飽和度的調整\n",
    "def expend1_img(img):\n",
    "    '''\n",
    "    :param img: 圖片數據\n",
    "    :return:\n",
    "    '''\n",
    "    a = random.random()\n",
    "    \n",
    "    t2 = transforms.RandomHorizontalFlip(p=0.5)  # 水平镜像，p是機率\n",
    "    t3 = transforms.RandomVerticalFlip(p=0.2) #垂直鏡像\n",
    "    # print(type(img))\n",
    "    img = t2(img)\n",
    "    img = t3(img)\n",
    "    if a<0.4:\n",
    "        t1 = transforms.RandomRotation(45)  # 随機旋轉，旋轉範圍為【-45,45】\n",
    "        img = t1(img)\n",
    "\n",
    "    t4 = transforms.ColorJitter(brightness=(0.8,1.5), contrast=(0.8,1.5), saturation=(0.8,1.5))#調整亮度  對比度  飽和度\n",
    "    img = t4(img)\n",
    "    return img\n",
    "\n",
    "#對圖片做透視轉換\n",
    "def expend2_img(img):\n",
    "    t = transforms.RandomPerspective(distortion_scale=0.6,p=1,interpolation = 2,fill=0) #圖片透視化\n",
    "    img2 = t(img)\n",
    "    return img2\n",
    "\n",
    "#切分訓練集和測試集，並進行補邊處理\n",
    "def split_train_test(img_dir,save_dir,train_val_num):\n",
    "    '''\n",
    "    :param img_dir: 原始图片路径，注意是所有类别所在文件夹的上一级目录\n",
    "    :param save_dir: 保存图片路径\n",
    "    :param train_val_num: 切分比例\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    img_dir_list=glob.glob(img_dir+os.sep+\"*\")#獲取每個類别所在的路徑（一個類别對應一個文件夾）\n",
    "    for class_dir in img_dir_list:\n",
    "        class_name=class_dir.split(os.sep)[-1] #獲取當前類别\n",
    "        img_list=glob.glob(class_dir+os.sep+\"*\") #獲取每個類别文件夾下的所有圖片\n",
    "        all_num=len(img_list) #獲取總個數\n",
    "        train_list=random.sample(img_list,int(all_num*train_val_num)) #訓練集圖片所在路徑\n",
    "        save_train=save_dir+os.sep+'train'+os.sep+class_name\n",
    "        save_val=save_dir+os.sep+\"val\"+os.sep+class_name\n",
    "        os.makedirs(save_train,exist_ok=True)\n",
    "        os.makedirs(save_val,exist_ok=True) #建立對應的文件夾\n",
    "        #保存切分好的數據集\n",
    "        for imgpath in img_list:\n",
    "            imgname=Path(imgpath).name #獲取文件名\n",
    "            if imgpath in train_list:\n",
    "                img=cv2.imread(imgpath)\n",
    "                new_img=expend_img(img)\n",
    "                cv2.imwrite(save_train+os.sep+imgname,new_img)\n",
    "            else: #將除了訓練集意外的數據均視為驗證集\n",
    "                img = cv2.imread(imgpath)\n",
    "                new_img = expend_img(img)\n",
    "                cv2.imwrite(save_val + os.sep + imgname, new_img)\n",
    "                \n",
    "    print(\"split train and val finished !\")\n",
    "\n",
    "#資料增強\n",
    "def data_enhancement(img_dir,save_dir,train_val_num):\n",
    "\n",
    "    img_dir_list=glob.glob(img_dir+os.sep+\"*\")#獲取每个類别所在的路徑（一個類别對應一個文件夾）\n",
    "    for class_dir in img_dir_list:\n",
    "        class_name=class_dir.split(os.sep)[-1] #獲取當前類别\n",
    "        img_list=glob.glob(class_dir+os.sep+\"*\") #獲取每個類别文件夾下的所有圖片\n",
    "        all_num=len(img_list) #獲取總個數\n",
    "        train_list=random.sample(img_list,int(all_num*train_val_num)) #訓練集圖片所在路徑\n",
    "        save_train=save_dir+os.sep+\"train\"+os.sep+class_name\n",
    "        save_val=save_dir+os.sep+\"val\"+os.sep+class_name\n",
    "        os.makedirs(save_train,exist_ok=True)\n",
    "        os.makedirs(save_val,exist_ok=True) #建立對應的文件夾\n",
    "        # print(class_name+\" trian num\",len(train_list))\n",
    "        # print(class_name+\" val num\",all_num-len(train_list))\n",
    "        #保存切分好的數據集\n",
    "        for imgpath in img_list:\n",
    "            imgname=Path(imgpath).name #獲取文件名\n",
    "            if imgpath in train_list:\n",
    "                img= Image.open(imgpath)\n",
    "                for time in range(3):\n",
    "                    new_img=expend1_img(img)\n",
    "                    Image.Image.save(new_img,save_train+os.sep+str(time)+imgname)\n",
    "\n",
    "#資料增強( 透視轉換 )\n",
    "def perspective_transform(img_dir,save_dir,train_val_num):\n",
    "    \n",
    "    img_dir_list=glob.glob(img_dir+os.sep+\"*\")#獲取每个類别所在的路徑（一個類别對應一個文件夾）\n",
    "    for class_dir in img_dir_list:\n",
    "        class_name=class_dir.split(os.sep)[-1] #獲取當前類别\n",
    "        img_list=glob.glob(class_dir+os.sep+\"*\") #獲取每個類别文件夾下的所有圖片\n",
    "        all_num=len(img_list) #獲取總個數\n",
    "        train_list=random.sample(img_list,int(all_num*train_val_num)) #訓練集圖片所在路徑\n",
    "        save_train=save_dir+os.sep+\"train\"+os.sep+class_name\n",
    "        save_val=save_dir+os.sep+\"val\"+os.sep+class_name\n",
    "        os.makedirs(save_train,exist_ok=True)\n",
    "        os.makedirs(save_val,exist_ok=True) #建立對應的文件夾\n",
    "        # print(class_name+\" trian num\",len(train_list))\n",
    "        # print(class_name+\" val num\",all_num-len(train_list))\n",
    "        #保存切分好的數據集\n",
    "        for imgpath in img_list:\n",
    "            imgname=Path(imgpath).name #獲取文件名\n",
    "            if imgpath in train_list:\n",
    "                img= Image.open(imgpath)\n",
    "                for time in range(1):\n",
    "                    img2 = expend2_img(img)\n",
    "                    Image.Image.save(img2,save_train+os.sep+str(time)+'_per_'+imgname)\n",
    "\n",
    "#efficientionnet訓練模型\n",
    "class Efficientnet_train():\n",
    "    def __init__(self,opt):\n",
    "        self.epochs=opt.epochs #訓練週期\n",
    "        self.batch_size=opt.batch_size #batch_size\n",
    "        self.class_num=opt.class_num #類别數\n",
    "        self.imgsz=opt.imgsz #圖片尺寸\n",
    "        self.img_dir=opt.img_dir #圖片路徑\n",
    "        self.weights=opt.weights #模型路徑\n",
    "        self.save_dir=opt.save_dir #保存模型路徑\n",
    "        self.save_model_name=opt.save_model_name #保存模型檔名\n",
    "        self.lr=opt.lr #初始化學習率\n",
    "        self.moment=opt.m #動量\n",
    "        base_model = EfficientNet.from_name('efficientnet-b5') #加載模型，使用b幾的就改為b幾\n",
    "        state_dict = torch.load(self.weights)\n",
    "        base_model.load_state_dict(state_dict)\n",
    "        # 修改全連接層\n",
    "        num_ftrs = base_model._fc.in_features\n",
    "        base_model._fc = nn.Linear(num_ftrs, self.class_num)\n",
    "        print(device)\n",
    "        self.model = base_model.to(device)\n",
    "        # 交叉熵損失函數\n",
    "        self.cross = nn.CrossEntropyLoss()\n",
    "        # 優化器\n",
    "        self.optimzer = optim.SGD((self.model.parameters()), lr=self.lr, momentum=self.moment, weight_decay=0.0004)\n",
    "\n",
    "        #獲取處理後的數據集和類别映射表\n",
    "        self.trainx,self.valx,self.b=self.process()\n",
    "        print(self.b)\n",
    "    def __call__(self):\n",
    "        best_acc = 0\n",
    "        self.model.train(True)\n",
    "        for ech in tqdm(range(self.epochs)):\n",
    "            optimzer1 = self.lrfn(ech, self.optimzer)\n",
    "\n",
    "            print(\"----------Start Train Epoch %d----------\" % (ech + 1))\n",
    "            # 開始訓練\n",
    "            run_loss = 0.0  # 損失\n",
    "            run_correct = 0.0  # 準確率\n",
    "            count = 0.0  # 分類正確的個數\n",
    "\n",
    "            for i, data in enumerate(self.trainx):\n",
    "                # print('train')\n",
    "                inputs, label = data\n",
    "                inputs, label = inputs.to(device), label.to(device)\n",
    "\n",
    "                # 訓練\n",
    "                optimzer1.zero_grad()\n",
    "                output = self.model(inputs)\n",
    "\n",
    "                loss = self.cross(output, label)\n",
    "                loss.backward()\n",
    "                optimzer1.step()\n",
    "\n",
    "                run_loss += loss.item()  # 損失累加\n",
    "                _, pred = torch.max(output.data, 1)\n",
    "                count += label.size(0)  # 求總共的訓練個數\n",
    "                run_correct += pred.eq(label.data).cpu().sum()  # 截止當前預測正確的個數\n",
    "                #每隔100個batch顯示一次信息，這裡顯示的ACC是當前預測正確的個數/當前訓練過的個數\n",
    "                if (i+1)%100==0:\n",
    "                    print('[Epoch:{}__iter:{}/{}] | Acc:{}'.format(ech + 1,i+1,len(self.trainx), run_correct/count))\n",
    "            # print(run_correct,'------------',count)\n",
    "            train_acc = run_correct / count\n",
    "            # 每次訓完一批顯示一次信息\n",
    "            print('Epoch:{} | Loss:{} | Acc:{}'.format(ech + 1, run_loss / len(self.trainx), train_acc))\n",
    "\n",
    "            # 訓完一批次後進行驗證\n",
    "            print(\"----------Waiting Test Epoch {}----------\".format(ech + 1))\n",
    "            with torch.no_grad():\n",
    "                correct = 0.  # 預測正確的個數\n",
    "                total = 0.  # 總個數\n",
    "                for inputs, labels in self.valx:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = self.model(inputs)\n",
    "\n",
    "                    # 穫取最高分的那個類的索引\n",
    "                    _, pred = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += pred.eq(labels).cpu().sum()\n",
    "                test_acc = correct / total\n",
    "                print(\"批次%d的验证集准确率\" % (ech + 1), correct / total)\n",
    "            if best_acc < test_acc:\n",
    "                best_acc = test_acc\n",
    "                start_time=(time.strftime(\"%m%d\",time.localtime()))\n",
    "                save_weight=self.save_dir+os.sep+start_time #保存路徑\n",
    "                os.makedirs(save_weight,exist_ok=True)\n",
    "                torch.save(self.model.state_dict(), save_weight + os.sep + self.save_model_name)#不加state_dict()存法會直接把模型架構和權重一起存入weight檔中\n",
    "                                                                                                       #加state_dict()則只單純存權重(不易報錯)\n",
    "\n",
    "  #數據處理\n",
    "    def process(self):\n",
    "        # 數據增强\n",
    "        data_transforms = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.Resize((self.imgsz, self.imgsz)),  # resize\n",
    "                transforms.CenterCrop((self.imgsz, self.imgsz)),  # 中心裁剪\n",
    "                transforms.RandomRotation(45),  # 随機旋轉，旋轉範圍為【-45,45】\n",
    "                transforms.ToTensor(),  # 轉換為張量\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 標準化\n",
    "            ]),\n",
    "            \"val\": transforms.Compose([\n",
    "                transforms.Resize((self.imgsz, self.imgsz)),  # resize\n",
    "                transforms.CenterCrop((self.imgsz, self.imgsz)),  # 中心裁剪\n",
    "                transforms.ToTensor(),  # 張量轉換\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        }\n",
    "\n",
    "        # 定義圖像生成器\n",
    "        image_datasets = {x: datasets.ImageFolder(root=os.path.join(self.img_dir,x), transform=data_transforms[x]) for x in ['train', 'val']}\n",
    "\n",
    "        # 得到訓練集和驗證集\n",
    "        trainx = DataLoader(image_datasets[\"train\"], batch_size=self.batch_size, shuffle=True, drop_last=True)\n",
    "        valx = DataLoader(image_datasets[\"val\"], batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "        b = image_datasets[\"train\"].class_to_idx  # id和類别對\n",
    "        return trainx,valx,b\n",
    "\n",
    "\n",
    "    # 學習率慢热加下降\n",
    "    def lrfn(self,num_epoch, optimzer):\n",
    "        lr_start = 0.00001  # 初始值\n",
    "        max_lr = 0.0004  # 最大值\n",
    "        lr_up_epoch = 10  # 學習率上升10个epoch\n",
    "        lr_sustain_epoch = 5  # 學習率保持不變\n",
    "        lr_exp = .8  # 衰减因子\n",
    "        if num_epoch < lr_up_epoch:  # 0-10个epoch學習率線性增加\n",
    "            lr = (max_lr - lr_start) / lr_up_epoch * num_epoch + lr_start\n",
    "        elif num_epoch < lr_up_epoch + lr_sustain_epoch:  # 學習率保持不變\n",
    "            lr = max_lr\n",
    "        else:  # 指數下降\n",
    "            lr = (max_lr - lr_start) * lr_exp ** (num_epoch - lr_up_epoch - lr_sustain_epoch) + lr_start\n",
    "        for param_group in optimzer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return optimzer\n",
    "\n",
    "#分類網路 function\n",
    "\n",
    "class Residual(nn.Module):  \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Residual, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=2, stride=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.conv4res = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=2, stride=1)\n",
    "        self.AvgPool1d=nn.AdaptiveAvgPool1d(12)\n",
    "        self.bn4res = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        Y1 = self.conv4res(X)\n",
    "        Y1 = self.bn4res(Y1)\n",
    "\n",
    "        Y2 = self.bn1(self.conv1(X))\n",
    "\n",
    "        Y2 = F.relu(Y2)\n",
    "\n",
    "        Y2 = self.bn2(self.conv2(Y2))\n",
    "\n",
    "        Y2 = F.relu(Y2)\n",
    "\n",
    "        return Y1 + Y2\n",
    "\n",
    "def resnet_block(in_channels, out_channels, num_residuals):\n",
    "\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        blk.append(Residual(in_channels, out_channels))\n",
    "\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "class LSTM_FCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layers):\n",
    "        super(LSTM_FCN, self).__init__()\n",
    "        # LSTM\n",
    "        self.conv4lstm = nn.Conv1d(48, 48, kernel_size=2, stride=2)\n",
    "        self.rnn = torch.nn.LSTM(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "\n",
    "        # 1D conv\n",
    "        self.conv1 = nn.Conv1d(1, 256, kernel_size=2, stride=2)\n",
    "        self.res_block=resnet_block(in_channels=256,out_channels=256,num_residuals=3)\n",
    "        self.conv2 = nn.Conv1d(256, 16, kernel_size=1, stride=1)\n",
    "        self.AvgPool1d=nn.AdaptiveAvgPool1d(48) # length\n",
    "\n",
    "        # concat softmax\n",
    "        self.fc1 = nn.Linear(768, 256, bias=True)\n",
    "        self.dropou1=nn.Dropout(0.7)\n",
    "        self.fc2 = nn.Linear(256, 64, bias=False)\n",
    "        self.dropou2=nn.Dropout(0.7)\n",
    "        self.fc3 = nn.Linear(64, output_dim, bias=False)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "\n",
    "    def init_model(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (torch.nn.Linear, torch.nn.Conv1d)):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "        print(\"init success !!\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # x=torch.unsqueeze(x,0)\n",
    "        # 1D cnn\n",
    "        cnn_out=self.conv1(x)\n",
    "        cnn_out=self.res_block(cnn_out)\n",
    "        cnn_out=self.conv2(cnn_out)\n",
    "        cnn_out=self.AvgPool1d(cnn_out)            \n",
    "\n",
    "        y=torch.flatten(cnn_out,start_dim=1)\n",
    "   \n",
    "        y = self.fc1(y) \n",
    "        y=self.dropou1(y)\n",
    "  \n",
    "        y = self.fc2(y) \n",
    "        y=self.dropou2(y)\n",
    "        y = self.fc3(y) \n",
    "\n",
    "        y= self.softmax(y)\n",
    " \n",
    "        return y\n",
    "\n",
    "'''算entropy'''\n",
    "def entropy(input):\n",
    "    all = 0\n",
    "    for t in range(len(input)):\n",
    "        if input[t]>=0:\n",
    "            en = -(input[t]*math.log(input[t],2))\n",
    "            all = all+en\n",
    "    return all\n",
    "\n",
    "\n",
    "# Load Test images\n",
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    def __init__(self, *args):\n",
    "        super(ImageFolderWithPaths, self).__init__(*args)\n",
    "        self.trans = args[1]\n",
    "    def __len__(self):\n",
    "      return len(self.imgs)\n",
    "    def __getitem__(self, index):\n",
    "        img, label = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        \n",
    "        path = self.imgs[index][0]\n",
    "        return (img, label ,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#過濾已經訓練過的data\n",
    "\n",
    "subset_path = 'recurrent_active_learning/cifar100/40percent/train'\n",
    "alldata_path = 'cifar_100/train_class/train'\n",
    "\n",
    "save_path = 'recurrent_active_learning/cifar100/60percent/train'\n",
    "\n",
    "class_id = os.listdir(subset_path)\n",
    "\n",
    "for c_id in class_id:\n",
    "    os.makedirs(save_path+'/'+c_id)\n",
    "\n",
    "for id in class_id:\n",
    "    subset_img = os.listdir(subset_path+'/'+id)\n",
    "    alldata_img = os.listdir(alldata_path+'/'+id)\n",
    "    for allimg_name in alldata_img:\n",
    "        get = 0\n",
    "        for subimg_name in subset_img:\n",
    "            if allimg_name == subimg_name:\n",
    "                get = 1\n",
    "        if get == 0:\n",
    "            img = cv2.imread(alldata_path+'/'+id+'/'+allimg_name)\n",
    "            cv2.imwrite(save_path+'/'+id+'/'+allimg_name, img)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'efficientnet_pytorch.model.EfficientNet'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taka\\anaconda3\\envs\\taka1\\lib\\site-packages\\ipykernel_launcher.py:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 ---- 30000 30000\n"
     ]
    }
   ],
   "source": [
    "#生成dict，內容 : ( 各類的cofidence , gt label  ) \n",
    "\n",
    "tensor = []\n",
    "label = []\n",
    "class_num = []\n",
    "path = []\n",
    "\n",
    "input_size = 224\n",
    "device = 'cuda'\n",
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "means = [0.485, 0.456, 0.406]\n",
    "stds = [0.229, 0.224, 0.225]\n",
    "unorm = UnNormalize(mean = means, std = stds)\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    def __init__(self, *args):\n",
    "        super(ImageFolderWithPaths, self).__init__(*args)\n",
    "        self.trans = args[1]\n",
    "    def __len__(self):\n",
    "      return len(self.imgs)\n",
    "    def __getitem__(self, index):\n",
    "        img, label = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        \n",
    "        path = self.imgs[index][0]\n",
    "        return (img, label ,path)\n",
    "# Load Test images\n",
    "def loaddata(data_dir, batch_size, set_name, shuffle):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(means, stds)\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(means, stds)\n",
    "        ]),\n",
    "        'test_class': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(means, stds)\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    image_datasets = {x: ImageFolderWithPaths(os.path.join(data_dir, x), data_transforms[x]) for x in [set_name]}\n",
    "    # num_workers=0 if CPU else = 1\n",
    "    dataset_loaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      shuffle=shuffle, num_workers=0) for x in [set_name]}\n",
    "    data_set_sizes = len(image_datasets[set_name])\n",
    "    return dataset_loaders, data_set_sizes\n",
    "\n",
    "\n",
    "def test_model(model, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    cont = 0\n",
    "    outPre = []\n",
    "    initi_tensor = []\n",
    "    outLabel = []\n",
    "    img_path = []\n",
    "    dset_loaders, dset_sizes = loaddata(data_dir=data_dir, batch_size=batch_size, set_name='train', shuffle=False)\n",
    "    transform = T.ToPILImage()\n",
    "    for data in dset_loaders['train']:\n",
    "        inputs, labels, paths = data #path抓出被分類的圖片的原始路徑\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        \n",
    "\n",
    "        # GPU\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        tensor.append(outputs.data)\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        class_num.append(labels)\n",
    "\n",
    "        path.append(paths)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        if cont == 0:\n",
    "            outPre = outputs.data.cpu()\n",
    "            outLabel = labels.data.cpu()\n",
    "        else:\n",
    "            outPre = torch.cat((outPre, outputs.data.cpu()), 0)\n",
    "            outLabel = torch.cat((outLabel, labels.data.cpu()), 0)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        cont += len(labels)\n",
    "        acc = running_corrects/cont\n",
    "\n",
    "    return FUN.softmax(Variable(outPre)).data.numpy(), outLabel.numpy()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    # Start Testing\n",
    "    net_name = 'efficientnet-b5'\n",
    "    data_dir = 'recurrent_active_learning/cifar100/60percent'\n",
    "    save_dir = 'recurrent_active_learning/weight/cifar100/0814'\n",
    "    modelft_file = save_dir + \"/\" + 'efficientb5_40percent' + '.pth'\n",
    "    batch_size = 1\n",
    "\n",
    "    # GPU時\n",
    "    model_ft = efficientnet_pytorch.EfficientNet.from_name(net_name)\n",
    "    # 修改全連接層\n",
    "    num_ftrs = model_ft._fc.in_features\n",
    "    model_ft._fc = nn.Linear(num_ftrs, 100)\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    model_ft.load_state_dict(torch.load(modelft_file))\n",
    "    print(type(model_ft))\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    outPre, outLabel = test_model(model_ft, criterion)\n",
    "\n",
    "print(str(len(tensor)),'----',str(len(class_num)),str(len(path)))\n",
    "\n",
    "with open('recurrent_active_learning/pickle/cifar100/cifar100_60percent_tensor.pickle', 'wb') as f:\n",
    "    pickle.dump(tensor, f)\n",
    "\n",
    "with open('recurrent_active_learning/pickle/cifar100/cifar100_60percent_class_label.pickle', 'wb') as f:\n",
    "    pickle.dump(class_num, f)\n",
    "\n",
    "with open('recurrent_active_learning/pickle/cifar100/cifar100_60percent_path.pickle', 'wb') as f:\n",
    "    pickle.dump(path, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taka\\anaconda3\\envs\\taka1\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "#用改完的confidence算enstropy挑 x% data(各類數量平均)\n",
    "\n",
    "with open('recurrent_active_learning/pickle/cifar100/cifar100_60percent_class_label.pickle', 'rb') as f:\n",
    "    label = pickle.load(f)\n",
    "\n",
    "with open('recurrent_active_learning/pickle/cifar100/cifar100_60percent_path.pickle', 'rb') as f:\n",
    "    path = pickle.load(f)\n",
    "\n",
    "with open('recurrent_active_learning/pickle/cifar100/cifar100_60percent_tensor.pickle', 'rb') as f:\n",
    "    result = pickle.load(f)\n",
    "\n",
    "class_num = 100\n",
    "\n",
    "\n",
    "\n",
    "img_save_dir = 'recurrent_active_learning/cifar100/five_10percent/train' \n",
    "img_dir = 'cifar_100/train_class/train'\n",
    "all_en = []\n",
    "for t in range(len(result)):\n",
    "    odds = FUN.softmax(Variable(result[t]).cpu()).data.numpy()\n",
    "    odds = odds.reshape([class_num])\n",
    "    data_entropy = entropy(odds)\n",
    "    all_en.append(data_entropy)\n",
    "sort = sorted(range(len(all_en)) , reverse = True,key = lambda k : all_en[k])\n",
    "\n",
    "img_dir_list=glob.glob(img_dir+os.sep+\"*\")#獲取每個類别所在的路徑（一個類别對應一個文件夾）\n",
    "for class_dir in img_dir_list:\n",
    "    class_name=class_dir.split(os.sep)[-1] #獲取當前類别\n",
    "    save_train=img_save_dir+os.sep+os.sep+class_name\n",
    "    os.makedirs(save_train,exist_ok=True)#建立對應的文件夾\n",
    "\n",
    "con = [0 for t in range(class_num)]\n",
    "for t in range(int(len(sort))):\n",
    "    p = re.sub(\"\\,\",\"\",str(path [sort[t]]))\n",
    "    p = re.sub(\"\\(\",\"\",p)\n",
    "    p = re.sub(\"\\)\",\"\",p)\n",
    "    p = re.sub(\"\\'\",\"\",p)\n",
    "    split = p.split('\\\\')\n",
    "\n",
    "    if con[int(split[4])] < 50:\n",
    "\n",
    "        con[int(split[4])] = con[int(split[4])]+1\n",
    "        img = cv2.imread(p)\n",
    "        cv2.imwrite(img_save_dir+'/'+ split[4]+'/'+split[6], img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把選出的資料合併進舊資料中\n",
    "\n",
    "old_path = 'recurrent_active_learning/cifar100/40percent/train'\n",
    "new_path = 'recurrent_active_learning/cifar100/five_10percent/train'\n",
    "save_path = 'recurrent_active_learning/cifar100/50percent/train'\n",
    "\n",
    "class_id = os.listdir(old_path)\n",
    "for c_id in class_id:\n",
    "    os.makedirs(save_path+'/'+c_id)\n",
    "\n",
    "for id in class_id:\n",
    "    oldimg_name = os.listdir(old_path+'/'+id)\n",
    "    newimg_name = os.listdir(new_path+'/'+id)\n",
    "    for oldimg in oldimg_name:\n",
    "        img = cv2.imread(old_path+'/'+id+'/'+oldimg)\n",
    "        cv2.imwrite(save_path+'/'+id+'/'+oldimg, img)\n",
    "    for newimg in newimg_name:\n",
    "        img = cv2.imread(new_path+'/'+id+'/'+newimg)\n",
    "        cv2.imwrite(save_path+'/'+id+'/'+newimg, img)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練efficientionnet\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#参數設置\n",
    "def parse_opt():\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--weights\",type=str,default=\"./model/efficientnet-b5-b6417697.pth\",help='initial weights path')#預訓練模型路徑\n",
    "    parser.add_argument(\"--img-dir\",type=str,default=\"./recurrent_active_learning/cifar100/50percent\",help=\"train image path\") #數據集的路徑\n",
    "    parser.add_argument(\"--imgsz\",type=int,default=224,help=\"image size\") #圖像尺寸\n",
    "    parser.add_argument(\"--epochs\",type=int,default=50,help=\"train epochs\")#訓練批次\n",
    "    parser.add_argument(\"--batch-size\",type=int,default=8,help=\"train batch-size\") #batch-size\n",
    "    parser.add_argument(\"--class_num\",type=int,default=100,help=\"class num\") #類別數\n",
    "    parser.add_argument(\"--lr\",type=float,default=0.0005,help=\"Init lr\") #學習率初始值\n",
    "    parser.add_argument(\"--m\",type=float,default=0.9,help=\"optimer momentum\") #動量\n",
    "    parser.add_argument(\"--save-dir\",type=str,default=\"./recurrent_active_learning/weight/cifar100\",help=\"save models dir\")#保存模型路徑\n",
    "\n",
    "    parser.add_argument(\"--save-model-name\",type=str,default=\"efficientb5_50percent.pth\",help=\"save models name\")#保存模型路徑\n",
    "    \n",
    "    opt=parser.parse_known_args()[0]\n",
    "    return opt\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    opt=parse_opt()\n",
    "    models=Efficientnet_train(opt)\n",
    "    models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('taka1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2932b575bb3a877f7f2eb58752af01fc0ca4cbf58bab91fbf4e91b10a7e35e59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
